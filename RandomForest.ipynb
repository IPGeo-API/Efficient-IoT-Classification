{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search on Random Forest\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "import keras\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, SCORERS\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.externals import joblib\n",
    "from operator import itemgetter\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Operating system version....', platform.platform())\n",
    "print(\"Python version is........... %s.%s.%s\" % sys.version_info[:3])\n",
    "print('scikit-learn version is.....', sklearn.__version__)\n",
    "print('pandas version is...........', pd.__version__)\n",
    "print('numpy version is............', np.__version__)\n",
    "print('matplotlib version is.......', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Time Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "  def __init__(self):\n",
    "    self.start = time.time()\n",
    "\n",
    "  def restart(self):\n",
    "    self.start = time.time()\n",
    "\n",
    "  def get_time(self):\n",
    "    end = time.time()\n",
    "    m, s = divmod(end - self.start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    return time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData():\n",
    "    global X_train, y_train, X_test, y_test, train, test \n",
    "    global feature_columns, response_column, n_features\n",
    "    subtract_mean = True\n",
    "    \n",
    "    hdf5_file = h5py.File('Data/dataset.hdf5', \"r\")\n",
    "    # read the training mean\n",
    "    if subtract_mean:\n",
    "        mm = hdf5_file[\"train_mean\"][...,0]\n",
    "        mm = mm[np.newaxis, ...]\n",
    "    # Total number of samples\n",
    "    data_num = hdf5_file[\"train_flow\"].shape[0]\n",
    "    flow_rows, flow_cols = 298, 17\n",
    "    X_train = hdf5_file[\"train_flow\"][...,0]\n",
    "    y_train = hdf5_file[\"train_labels\"][:,...]\n",
    "    X_test = hdf5_file[\"test_flow\"][...,0]\n",
    "    y_test = hdf5_file[\"test_labels\"][:,...]\n",
    "    hdf5_file.close()\n",
    "    X_train = np.concatenate((X_train,X_test),axis=0)\n",
    "    if subtract_mean:\n",
    "        X_train -= mm\n",
    "    y_train = np.concatenate((y_train,y_test),axis=0)\n",
    "    train_shape = (X_train.shape[0], 298, 17, 1)\n",
    "    \n",
    "    hdf5_file = h5py.File('Data/dataset-IoT.hdf5', \"r\")\n",
    "    # Total number of samples\n",
    "    data_num_test = hdf5_file[\"IoT_flow\"].shape[0]\n",
    "    X_test = hdf5_file[\"IoT_flow\"][...,0]\n",
    "    if subtract_mean:\n",
    "        X_test -= mm\n",
    "    y_test = hdf5_file[\"labels\"][:,...]\n",
    "    hdf5_file.close()\n",
    "    \n",
    "    X_test = np.nan_to_num(X_test)\n",
    "    print(np.isnan(X_test).sum())\n",
    "\n",
    "    X_train =np.reshape(X_train,(X_train.shape[0],-1))\n",
    "    X_test =np.reshape(X_test,(X_test.shape[0],-1))\n",
    "    \n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_Curve(rf, auc):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    rf_fit = rf.fit(X_train, y_train)\n",
    "    fit = one_hot_encoder.fit(rf.apply(X_train))\n",
    "    y_predicted = rf.predict_proba(X_test)[:, 1]\n",
    "    false_positive, true_positive, threshold = roc_curve(y_test, y_predicted)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(false_positive, true_positive, color='darkorange', label='Random Forest')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve (area = %0.2f)' % auc)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_Metrics(saved_rf):\n",
    "    print('\\nModel performance on the test data set:')\n",
    "\n",
    "    # print('Train Accuracy.......', accuracy_score(y_train, best_model.predict(X_train)))\n",
    "    # print('Validate Accuracy....', accuracy_score(y_valid, best_model.predict(X_valid)))\n",
    "\n",
    "    y_predict_test  = best_model.predict(X_test)\n",
    "    mse             = metrics.mean_squared_error(y_test, y_predict_test)\n",
    "    logloss_test    = metrics.log_loss(y_test, y_predict_test)\n",
    "    accuracy_test   = metrics.accuracy_score(y_test, y_predict_test)\n",
    "    accuracy_test2  = best_model.score(X_test, y_test)\n",
    "    F1_test         = metrics.f1_score(y_test, y_predict_test)\n",
    "    precision_test  = precision_score(y_test, y_predict_test, average='binary')\n",
    "    precision_test2 = metrics.precision_score(y_test, y_predict_test)\n",
    "    recall_test     = recall_score(y_test, y_predict_test, average='binary')\n",
    "    auc_test        = metrics.roc_auc_score(y_test, y_predict_test)\n",
    "    r2_test         = metrics.r2_score(y_test, y_predict_test)\n",
    "   \n",
    "    #test_auc       = h2o.get_model(\"best_rf\").model_performance(test_data=test).auc()\n",
    "    #print('Best model performance based on auc: ', test_auc)\n",
    "    \n",
    "    header = [\"Metric\", \"Test\"]\n",
    "    table  = [\n",
    "               [\"logloss\",   logloss_test],\n",
    "               [\"accuracy\",  accuracy_test],\n",
    "               [\"precision\", precision_test],\n",
    "               [\"F1\",        F1_test],\n",
    "               [\"r2\",        r2_test],\n",
    "               [\"AUC\",       auc_test]\n",
    "             ]\n",
    "\n",
    "    print(tabulate(table, header, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Score Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Report_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_confusion_matrix(cm, auc, heading):\n",
    "    print('\\n', heading)\n",
    "    print(cm)\n",
    "    true_negative  = cm[0,0]\n",
    "    true_positive  = cm[1,1]\n",
    "    false_negative = cm[1,0]\n",
    "    false_positive = cm[0,1]\n",
    "    total = true_negative + true_positive + false_negative + false_positive\n",
    "    accuracy = (true_positive + true_negative)/total\n",
    "    precision = (true_positive)/(true_positive + false_positive)\n",
    "    recall = (true_positive)/(true_positive + false_negative)\n",
    "    misclassification_rate = (false_positive + false_negative)/total\n",
    "    F1 = (2*true_positive)/(2*true_positive + false_positive + false_negative)\n",
    "    print('accuracy.................%7.4f' % accuracy)\n",
    "    print('precision................%7.4f' % precision)\n",
    "    print('recall...................%7.4f' % recall)\n",
    "    print('F1.......................%7.4f' % F1)\n",
    "    print('auc......................%7.4f' % auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learining Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_learning_curve(estimator, title, X, y, ylim = None, cv = None,\n",
    "                        n_jobs = 1, train_sizes = np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
    "                                                            X, y,\n",
    "                                                            cv = cv,\n",
    "                                                            n_jobs = n_jobs,\n",
    "                                                            train_sizes = train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Search and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Search():\n",
    "    global best_model, saved_model\n",
    "    \n",
    "    param_grid = {\"n_estimators\": range(20, 100, 20),\n",
    "                  \"max_depth\": range(4, 20, 4),\n",
    "                  \"min_samples_leaf\": range(2, 100, 10),\n",
    "                  \"min_samples_split\": sp_randint(2, 10),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}   \n",
    "\n",
    "    clf = RandomForestClassifier(class_weight = 'balanced')\n",
    "    n_iter_search = 200\n",
    "    estimator = RandomizedSearchCV(clf,\n",
    "                                   param_distributions = param_grid,\n",
    "                                   n_iter = n_iter_search,\n",
    "                                   scoring = 'neg_log_loss',\n",
    "                                   verbose = 0,\n",
    "                                   n_jobs = 32)\n",
    "        \n",
    "    fit = estimator.fit(X_train, y_train)\n",
    "\n",
    "    # Cross validation with 20 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    cv_ = ShuffleSplit(n_splits = 20, test_size = 0.20, random_state = 0)\n",
    "    Plot_learning_curve(estimator, \n",
    "                        'Learning Curves',\n",
    "                        X_train, y_train, \n",
    "                        cv = cv_,\n",
    "                        n_jobs = 32)\n",
    "     \n",
    "    Report_scores(estimator.cv_results_, n_top = 3)\n",
    "    \n",
    "    best_model = estimator.best_estimator_\n",
    "    print('\\nbest_model:\\n', best_model)\n",
    "\n",
    "    print('\\nFeature Importances:', best_model.feature_importances_)\n",
    "    #Plot_predictor_importance(best_model, feature_columns)\n",
    "\n",
    "    y_predicted = best_model.predict(X_train)\n",
    "    probabilities = best_model.predict_proba(X_train)\n",
    "\n",
    "    c_report = classification_report(y_train, y_predicted)\n",
    "    print('\\nClassification report:\\n', c_report)\n",
    "\n",
    "    y_predicted_train = best_model.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_predicted_train)\n",
    "    auc = roc_auc_score(y_train, y_predicted_train)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the training dataset')\n",
    "\n",
    "    y_predicted = best_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "    auc = roc_auc_score(y_test, y_predicted)\n",
    "\n",
    "    ntotal = len(y_test)\n",
    "    correct = y_test == y_predicted\n",
    "    numCorrect = sum(correct)\n",
    "    percent = round( (100.0*numCorrect)/ntotal, 6)\n",
    "    print(\"\\nCorrect classifications on test data: {0:d}/{1:d} {2:8.3f}%\".format(numCorrect, ntotal, percent))\n",
    "    prediction_score = 100.0*best_model.score(X_test, y_test)\n",
    "    print('Random Forest Prediction Score on test data: %8.3f' % prediction_score)\n",
    "\n",
    "    model_path = 'sklearn_rf_classify.pkl'\n",
    "    joblib.dump(best_model, model_path)\n",
    "\n",
    "    saved_model = joblib.load(model_path)\n",
    "    y_predicted_test = best_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predicted_test)\n",
    "    auc = roc_auc_score(y_test, y_predicted_test)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the test dataset')\n",
    "    ROC_Curve(best_model, auc)\n",
    "    Print_Metrics(saved_model)\n",
    "    \n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    rf_fit = best_model.fit(X_train, y_train)\n",
    "    fit = one_hot_encoder.fit(best_model.apply(X_train))\n",
    "    y_predicted = best_model.predict_proba(X_test)[:, 1]\n",
    "    false_positive, true_positive, threshold = roc_curve(y_test, y_predicted)\n",
    "    pd.DataFrame({\"FalsePositiveRate\" : false_positive, \"TruePositiveRate\" :true_positive, \"Threshold\" : threshold}).to_csv(\"RF-ROC.csv\", index=None)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_timer = Timer()\n",
    "LoadData()\n",
    "Random_Search()\n",
    "elapsed = my_timer.get_time()\n",
    "print(\"\\nTotal compute time was: %s\" % elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
