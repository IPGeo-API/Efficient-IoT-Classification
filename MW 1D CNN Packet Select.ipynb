{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "import gc\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path = 'Data/dataset.hdf5'\n",
    "subtract_mean = True\n",
    "\n",
    "hdf5_file = h5py.File(hdf5_path, \"r\")\n",
    "\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file[\"train_mean\"][...,0]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "\n",
    "data_num = hdf5_file[\"train_flow\"].shape[0]\n",
    "    \n",
    "num_classes = 2\n",
    "epochs = 30\n",
    "\n",
    "flow_rows, flow_cols = 298, 17\n",
    "\n",
    "x_train = hdf5_file[\"train_flow\"][...,0]\n",
    "if subtract_mean:\n",
    "    x_train -= mm\n",
    "\n",
    "y_train = hdf5_file[\"train_labels\"][:,...]\n",
    "hdf5_file.close()\n",
    "hdf5_path = 'Data/dataset-IoT.hdf5'\n",
    "hdf5_file = h5py.File(hdf5_path, \"r\")\n",
    "\n",
    "\n",
    "x_test = hdf5_file[\"IoT_flow\"][...,0]\n",
    "if subtract_mean:\n",
    "    x_test -= mm\n",
    "\n",
    "y_test = hdf5_file[\"labels\"][:,...]\n",
    "\n",
    "hdf5_file.close()\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "#define number of packets used to train for each device\n",
    "packets = 200\n",
    "x_train = x_train[:,:packets,:]\n",
    "x_test = x_test[:,:packets,:]\n",
    "\n",
    "input_shape = (x_train.shape[1], x_train.shape[2])\n",
    "    \n",
    "    \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AccScores = []\n",
    "Times = []\n",
    "LossScores = []\n",
    "#AucScores = []\n",
    "#F1Scores = []\n",
    "#PrecisionScores = []\n",
    "t2 = time.time()\n",
    "for i in range (1,50):\n",
    "    num_classes = 2\n",
    "    high = 80\n",
    "    \n",
    "    filter_lenghts =  [int(i) for i in np.arange(2,high,2)]\n",
    "    #print(filter_lenghts)\n",
    "    convs = []\n",
    "    maxlen = packets\n",
    "    batch_size = 1024\n",
    "    epochs = 30\n",
    "    numFilters=128\n",
    "    activations= \"sigmoid\"\n",
    "    dropoutVal = 0.2485206320388983\n",
    "    lr = 0.001362881122433337\n",
    "    adam = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "\n",
    "    optim = adam\n",
    "    \n",
    "    input_flow = Input(shape=input_shape)\n",
    "\n",
    "    convs= {}\n",
    "    mpoolings = {}\n",
    "    flattens = {}\n",
    "    convs_out = []\n",
    "    for i in filter_lenghts:\n",
    "        convs[str(i)+'_convolution']=Conv1D(filters=numFilters,kernel_size=i,padding=\"valid\",activation=activations,strides=1)(input_flow)\n",
    "\n",
    "        mpoolings[str(i)+'_maxpooling'] = MaxPooling1D(pool_size= maxlen - i + 1)(convs[str(i)+'_convolution'])\n",
    "        flattens[str(i)+'_flattenout'] = Flatten()(mpoolings[str(i)+'_maxpooling'])\n",
    "        convs_out.append(flattens[str(i)+'_flattenout'])\n",
    "    out = Concatenate()(convs_out)\n",
    "    dropout = Dropout(dropoutVal)(out)\n",
    "    dense = Dense(64, activation='relu')(dropout)\n",
    "    dense2 = Dense(32, activation='relu')(dense)\n",
    "    dropout2 = Dropout(dropoutVal)(dense2)\n",
    "    end = Dense(num_classes, activation='softmax')(dropout2)\n",
    "\n",
    "    model = Model(inputs=input_flow, outputs=end) \n",
    "    #model.summary()\n",
    "    try:\n",
    "        model = multi_gpu_model(model, gpus = 4)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    t0 = time.time()\n",
    "    model.fit(x_train,y_train, batch_size=batch_size, epochs=epochs, verbose=0, class_weight=class_weights, shuffle=True)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    t1 = time.time()\n",
    "    total_run = t1-t0\n",
    "    Times.append(total_run)\n",
    "    print(\"%s: %.5f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.5f%%\" % (model.metrics_names[0], scores[0]))\n",
    "    print('Fit and Evaluate Time:',total_run)\n",
    "    AccScores.append(scores[1] * 100)\n",
    "    LossScores.append(scores[0])\n",
    "    del model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "print(\"%.5f%% (+/- %.5f%%)\" % (np.mean(AccScores), np.std(AccScores)))\n",
    "print(\"%.5f%% (+/- %.5f%%)\" % (np.mean(LossScores), np.std(LossScores)))\n",
    "print(\"%.5f%% (+/- %.5f%%)\" % (np.mean(Times), np.std(Times)))\n",
    "#print(\"%.5f%% (+/- %.5f%%)\" % (np.mean(AucScores), np.std(AucScores)))\n",
    "#print(\"%.5f%% (+/- %.5f%%)\" % (np.mean(F1Scores), np.std(F1Scores)))\n",
    "#print(\"%.5f%% (+/- %.5f%%)\" % (np.mean(PrecisionScores), np.std(PrecisionScores)))\n",
    "t3 = time.time()\n",
    "total_time = t3-t2\n",
    "print('Time to do everything:',total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AccScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
